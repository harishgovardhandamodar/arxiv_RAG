{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArXiv RAG Search System Using FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources to learn RAG basics\n",
    "\n",
    "Here's a list of resources that will help you understand what's happening in this notebook:\n",
    "\n",
    "- Faiss - Introduction to Similarity Search<br>\n",
    "James Briggs<br>\n",
    "https://www.youtube.com/watch?v=sKyvsdEv6rk\n",
    "\n",
    "- Large Language Models with Semantic Search<br>\n",
    "Deeplearning.Ai short course<br>\n",
    "https://www.deeplearning.ai/short-courses/large-language-models-semantic-search/\n",
    "\n",
    "- Colab Notebook that explains rerank<br>\n",
    "retrieve_rerank_simple_wikipedia.ipynb<br>\n",
    "https://colab.research.google.com/github/UKPLab/sentence-transformers/blob/master/examples/applications/retrieve_rerank/retrieve_rerank_simple_wikipedia.ipynb#scrollTo=UlArb7kqN3Re\n",
    "\n",
    "- Vector Databases: from Embeddings to Applications<br>\n",
    "Deeplearning.Ai short course<br>\n",
    "(This approach is an alternative to using FAISS and Sentence Transformers)<br>\n",
    "https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/\n",
    "\n",
    "- Sentence transformers docs<br>\n",
    "https://www.sbert.net/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Build a RAG (Retrieval Augmented Generation) search system that uses vector search to compare natural language search queries to research paper titles and abstracts in the ArXiv database.\n",
    "- Use free tools like Sentence Transformers to create the vectors and FAISS to manage the vector search.\n",
    "- Format the search results to enable quick review.\n",
    "- Use OpenAi to create a natural language ouput.\n",
    "\n",
    "##### Example search query: \"I want to build an invisibility cloak like the one in Harry Potter.\"\n",
    "\n",
    "## Approach\n",
    "\n",
    "Arxiv has a dataset on Kaggle that gets updated weekly. It includes a file containing metadata for all papers stored in the Arxiv database. The metadata includes the title and abstract of each paper. There are approximately 2.4 million papers in the ArXiv dataset.\n",
    "\n",
    "To create the RAG system we will take the title and abstract for each paper and convert it into a vector embedding. These vectors will be stored in a FAISS index.\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is an open-source library designed for fast (GPU supported) vector similarity search in large datasets.\n",
    "\n",
    "When a search query is submitted, the query text string will be vectorized. This query vector will then be compared to the vectors in the FAISS index. Then, the search results will be reranked and the top matches will be returned. The paper title, arxiv categories, paper abstract and a link to the pdf file will be displayed for each search result. This format will enable users to quickly scan through the search results to determine which papers are relevant to their work.\n",
    "\n",
    "Finally, an OpenAi model wil be used to create a one sentence summary of each abstract in the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (4.37.2)\n",
      "Requirement already satisfied: tqdm in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m162.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.7.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (1.3 kB)\n",
      "Using cached faiss_cpu-1.7.4-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "#!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from openai) (3.7.1)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from openai) (0.23.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from openai) (2.6.1)\n",
      "Requirement already satisfied: sniffio in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (0.15.0)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx<1,>=0.23.0->openai) (1.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<1,>=0.23.0->openai) (0.12.0)\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m253.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, openai\n",
      "Successfully installed distro-1.9.0 openai-1.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /Users/harishdamodar/anaconda3/envs/reverseengineer/lib/python3.9/site-packages (from rank-bm25) (1.24.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Please add your OpenAi API Key here.\n",
    "# I will delete this key after committing this notebook.\n",
    "#OPENAI_API_KEY  = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'arxiv-dataset.zip', 'Vector', 'arxiv-dataset']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./arxiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Arxiv category codes\n",
    "# Source: https://www.kaggle.com/code/artgor/arxiv-metadata-exploration\n",
    "\n",
    "# https://arxiv.org/category_taxonomy\n",
    "# https://info.arxiv.org/help/api/user-manual.html#subject_classifications\n",
    "\n",
    "\n",
    "category_map = {\n",
    "# These created errors when mapping categories to descriptions\n",
    "'acc-phys': 'Accelerator Physics',\n",
    "'adap-org': 'Not available',\n",
    "'q-bio': 'Not available',\n",
    "'cond-mat': 'Not available',\n",
    "'chao-dyn': 'Not available',\n",
    "'patt-sol': 'Not available',\n",
    "'dg-ga': 'Not available',\n",
    "'solv-int': 'Not available',\n",
    "'bayes-an': 'Not available',\n",
    "'comp-gas': 'Not available',\n",
    "'alg-geom': 'Not available',\n",
    "'funct-an': 'Not available',\n",
    "'q-alg': 'Not available',\n",
    "'ao-sci': 'Not available',\n",
    "'atom-ph': 'Atomic Physics',\n",
    "'chem-ph': 'Chemical Physics',\n",
    "'plasm-ph': 'Plasma Physics',\n",
    "'mtrl-th': 'Not available',\n",
    "'cmp-lg': 'Not available',\n",
    "'supr-con': 'Not available',\n",
    "###\n",
    "\n",
    "# Added\n",
    "'econ.GN': 'General Economics',\n",
    "'econ.TH': 'Theoretical Economics',\n",
    "'eess.SY': 'Systems and Control',\n",
    "\n",
    "'astro-ph': 'Astrophysics',\n",
    "'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "'cond-mat.mtrl-sci': 'Materials Science',\n",
    "'cond-mat.other': 'Other Condensed Matter',\n",
    "'cond-mat.quant-gas': 'Quantum Gases',\n",
    "'cond-mat.soft': 'Soft Condensed Matter',\n",
    "'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "'cond-mat.supr-con': 'Superconductivity',\n",
    "'cs.AI': 'Artificial Intelligence',\n",
    "'cs.AR': 'Hardware Architecture',\n",
    "'cs.CC': 'Computational Complexity',\n",
    "'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "'cs.CG': 'Computational Geometry',\n",
    "'cs.CL': 'Computation and Language',\n",
    "'cs.CR': 'Cryptography and Security',\n",
    "'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "'cs.CY': 'Computers and Society',\n",
    "'cs.DB': 'Databases',\n",
    "'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "'cs.DL': 'Digital Libraries',\n",
    "'cs.DM': 'Discrete Mathematics',\n",
    "'cs.DS': 'Data Structures and Algorithms',\n",
    "'cs.ET': 'Emerging Technologies',\n",
    "'cs.FL': 'Formal Languages and Automata Theory',\n",
    "'cs.GL': 'General Literature',\n",
    "'cs.GR': 'Graphics',\n",
    "'cs.GT': 'Computer Science and Game Theory',\n",
    "'cs.HC': 'Human-Computer Interaction',\n",
    "'cs.IR': 'Information Retrieval',\n",
    "'cs.IT': 'Information Theory',\n",
    "'cs.LG': 'Machine Learning',\n",
    "'cs.LO': 'Logic in Computer Science',\n",
    "'cs.MA': 'Multiagent Systems',\n",
    "'cs.MM': 'Multimedia',\n",
    "'cs.MS': 'Mathematical Software',\n",
    "'cs.NA': 'Numerical Analysis',\n",
    "'cs.NE': 'Neural and Evolutionary Computing',\n",
    "'cs.NI': 'Networking and Internet Architecture',\n",
    "'cs.OH': 'Other Computer Science',\n",
    "'cs.OS': 'Operating Systems',\n",
    "'cs.PF': 'Performance',\n",
    "'cs.PL': 'Programming Languages',\n",
    "'cs.RO': 'Robotics',\n",
    "'cs.SC': 'Symbolic Computation',\n",
    "'cs.SD': 'Sound',\n",
    "'cs.SE': 'Software Engineering',\n",
    "'cs.SI': 'Social and Information Networks',\n",
    "'cs.SY': 'Systems and Control',\n",
    "'econ.EM': 'Econometrics',\n",
    "'eess.AS': 'Audio and Speech Processing',\n",
    "'eess.IV': 'Image and Video Processing',\n",
    "'eess.SP': 'Signal Processing',\n",
    "'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "'hep-ex': 'High Energy Physics - Experiment',\n",
    "'hep-lat': 'High Energy Physics - Lattice',\n",
    "'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "'hep-th': 'High Energy Physics - Theory',\n",
    "'math.AC': 'Commutative Algebra',\n",
    "'math.AG': 'Algebraic Geometry',\n",
    "'math.AP': 'Analysis of PDEs',\n",
    "'math.AT': 'Algebraic Topology',\n",
    "'math.CA': 'Classical Analysis and ODEs',\n",
    "'math.CO': 'Combinatorics',\n",
    "'math.CT': 'Category Theory',\n",
    "'math.CV': 'Complex Variables',\n",
    "'math.DG': 'Differential Geometry',\n",
    "'math.DS': 'Dynamical Systems',\n",
    "'math.FA': 'Functional Analysis',\n",
    "'math.GM': 'General Mathematics',\n",
    "'math.GN': 'General Topology',\n",
    "'math.GR': 'Group Theory',\n",
    "'math.GT': 'Geometric Topology',\n",
    "'math.HO': 'History and Overview',\n",
    "'math.IT': 'Information Theory',\n",
    "'math.KT': 'K-Theory and Homology',\n",
    "'math.LO': 'Logic',\n",
    "'math.MG': 'Metric Geometry',\n",
    "'math.MP': 'Mathematical Physics',\n",
    "'math.NA': 'Numerical Analysis',\n",
    "'math.NT': 'Number Theory',\n",
    "'math.OA': 'Operator Algebras',\n",
    "'math.OC': 'Optimization and Control',\n",
    "'math.PR': 'Probability',\n",
    "'math.QA': 'Quantum Algebra',\n",
    "'math.RA': 'Rings and Algebras',\n",
    "'math.RT': 'Representation Theory',\n",
    "'math.SG': 'Symplectic Geometry',\n",
    "'math.SP': 'Spectral Theory',\n",
    "'math.ST': 'Statistics Theory',\n",
    "'math-ph': 'Mathematical Physics',\n",
    "'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "'nlin.CD': 'Chaotic Dynamics',\n",
    "'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "'nlin.PS': 'Pattern Formation and Solitons',\n",
    "'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "'nucl-ex': 'Nuclear Experiment',\n",
    "'nucl-th': 'Nuclear Theory',\n",
    "'physics.acc-ph': 'Accelerator Physics',\n",
    "'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "'physics.app-ph': 'Applied Physics',\n",
    "'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "'physics.atom-ph': 'Atomic Physics',\n",
    "'physics.bio-ph': 'Biological Physics',\n",
    "'physics.chem-ph': 'Chemical Physics',\n",
    "'physics.class-ph': 'Classical Physics',\n",
    "'physics.comp-ph': 'Computational Physics',\n",
    "'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "'physics.ed-ph': 'Physics Education',\n",
    "'physics.flu-dyn': 'Fluid Dynamics',\n",
    "'physics.gen-ph': 'General Physics',\n",
    "'physics.geo-ph': 'Geophysics',\n",
    "'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "'physics.ins-det': 'Instrumentation and Detectors',\n",
    "'physics.med-ph': 'Medical Physics',\n",
    "'physics.optics': 'Optics',\n",
    "'physics.plasm-ph': 'Plasma Physics',\n",
    "'physics.pop-ph': 'Popular Physics',\n",
    "'physics.soc-ph': 'Physics and Society',\n",
    "'physics.space-ph': 'Space Physics',\n",
    "'q-bio.BM': 'Biomolecules',\n",
    "'q-bio.CB': 'Cell Behavior',\n",
    "'q-bio.GN': 'Genomics',\n",
    "'q-bio.MN': 'Molecular Networks',\n",
    "'q-bio.NC': 'Neurons and Cognition',\n",
    "'q-bio.OT': 'Other Quantitative Biology',\n",
    "'q-bio.PE': 'Populations and Evolution',\n",
    "'q-bio.QM': 'Quantitative Methods',\n",
    "'q-bio.SC': 'Subcellular Processes',\n",
    "'q-bio.TO': 'Tissues and Organs',\n",
    "'q-fin.CP': 'Computational Finance',\n",
    "'q-fin.EC': 'Economics',\n",
    "'q-fin.GN': 'General Finance',\n",
    "'q-fin.MF': 'Mathematical Finance',\n",
    "'q-fin.PM': 'Portfolio Management',\n",
    "'q-fin.PR': 'Pricing of Securities',\n",
    "'q-fin.RM': 'Risk Management',\n",
    "'q-fin.ST': 'Statistical Finance',\n",
    "'q-fin.TR': 'Trading and Market Microstructure',\n",
    "'quant-ph': 'Quantum Physics',\n",
    "'stat.AP': 'Applications',\n",
    "'stat.CO': 'Computation',\n",
    "'stat.ME': 'Methodology',\n",
    "'stat.ML': 'Machine Learning',\n",
    "'stat.OT': 'Other Statistics',\n",
    "'stat.TH': 'Statistics Theory'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2426574, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>hep-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \n",
       "0    A fully differential calculation in perturba...           hep-ph  \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG  \n",
       "2    The evolution of Earth-Moon system is descri...   physics.gen-ph  \n",
       "3    We show that a determinant of Stirling cycle...          math.CO  \n",
       "4    In this paper we show how to compute the $\\L...  math.CA math.FA  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/code/matthewmaddock/nlp-arxiv-dataset-transformers-and-umap\n",
    "\n",
    "# This takes about 1 minute.\n",
    "\n",
    "\n",
    "cols = ['id', 'title', 'abstract', 'categories']\n",
    "data = []\n",
    "file_name = './arxiv/arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "\n",
    "with open(file_name, encoding='latin-1') as f:\n",
    "    for line in f:\n",
    "        doc = json.loads(line)\n",
    "        lst = [doc['id'], doc['title'], doc['abstract'], doc['categories']]\n",
    "        data.append(lst)\n",
    "\n",
    "df_data = pd.DataFrame(data=data, columns=cols)\n",
    "\n",
    "print(df_data.shape)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the category codes into text\n",
    "\n",
    "If a description was not available for a category code then I left it out when converting category codes into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>cat_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>High Energy Physics - Phenomenology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>Combinatorics, Computational Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>General Physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>Combinatorics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>Classical Analysis and ODEs, Functional Analysis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0  0704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  0704.0002           Sparsity-certifying Graph Decompositions   \n",
       "2  0704.0003  The evolution of the Earth-Moon system based o...   \n",
       "3  0704.0004  A determinant of Stirling cycle numbers counts...   \n",
       "4  0704.0005  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                            abstract       categories  \\\n",
       "0    A fully differential calculation in perturba...           hep-ph   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...    math.CO cs.CG   \n",
       "2    The evolution of Earth-Moon system is descri...   physics.gen-ph   \n",
       "3    We show that a determinant of Stirling cycle...          math.CO   \n",
       "4    In this paper we show how to compute the $\\L...  math.CA math.FA   \n",
       "\n",
       "                                           cat_text  \n",
       "0               High Energy Physics - Phenomenology  \n",
       "1             Combinatorics, Computational Geometry  \n",
       "2                                   General Physics  \n",
       "3                                     Combinatorics  \n",
       "4  Classical Analysis and ODEs, Functional Analysis  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cat_text(x):\n",
    "\n",
    "    cat_text = ''\n",
    "\n",
    "    # Put the codes into a list\n",
    "    cat_list = x.split(' ')\n",
    "\n",
    "    for i, item in enumerate(cat_list):\n",
    "\n",
    "        cat_name = category_map[item]\n",
    "\n",
    "        # If there was no description available\n",
    "        # for the category code then don't include it in the text.\n",
    "        if cat_name != 'Not available':\n",
    "\n",
    "            if i == 0:\n",
    "                cat_text = cat_name\n",
    "            else:\n",
    "                cat_text = cat_text + ', ' + cat_name\n",
    "\n",
    "    # Remove leading and trailing spaces\n",
    "    cat_text = cat_text.strip()\n",
    "\n",
    "    return cat_text\n",
    "\n",
    "\n",
    "df_data['cat_text'] = df_data['categories'].apply(get_cat_text)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: 0704.0002\n",
      "\n",
      "Title: Sparsity-certifying Graph Decompositions\n",
      "\n",
      "Categories: Combinatorics, Computational Geometry\n",
      "\n",
      "Abstract:   We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use\n",
      "it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and\n",
      "algorithmic solutions to a family of problems concerning tree decompositions of\n",
      "graphs. Special instances of sparse graphs appear in rigidity theory and have\n",
      "received increased attention in recent years. In particular, our colored\n",
      "pebbles generalize and strengthen the previous results of Lee and Streinu and\n",
      "give a new proof of the Tutte-Nash-Williams characterization of arboricity. We\n",
      "also present a new decomposition that certifies sparsity based on the\n",
      "$(k,\\ell)$-pebble game with colors. Our work also exposes connections between\n",
      "pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\n",
      "Westermann and Hendrickson.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print details of one paper\n",
    "\n",
    "i = 1\n",
    "\n",
    "print('Id:',df_data.loc[i, 'id'])\n",
    "print()\n",
    "print('Title:',df_data.loc[i, 'title'])\n",
    "print()\n",
    "print('Categories:',df_data.loc[i, 'cat_text'])\n",
    "print()\n",
    "print('Abstract:',df_data.loc[i, 'abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the text\n",
    "- Replace newline characters ('\\n') with a space\n",
    "- Remove leading and trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace newline characters ('\\n') with a space\n",
    "# Remove leading and trailing spaces\n",
    "\n",
    "def clean_text(x):\n",
    "\n",
    "    # Replace newline characters with a space\n",
    "    new_text = x.replace(\"\\n\", \" \")\n",
    "    # Remove leading and trailing spaces\n",
    "    new_text = new_text.strip()\n",
    "\n",
    "    return new_text\n",
    "\n",
    "df_data['title'] = df_data['title'].apply(clean_text)\n",
    "df_data['abstract'] = df_data['abstract'].apply(clean_text)\n",
    "\n",
    "#df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the text string that will be vectorized\n",
    "\n",
    "Here the title will be appended to the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the title to the abstract\n",
    "\n",
    "df_data['prepared_text'] = df_data['title'] + ' {title} ' + df_data['abstract']\n",
    "\n",
    "#df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data ready for vectorizing\n",
    "\n",
    "We need a list of text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426574\n",
      "2426574\n",
      "2426574\n"
     ]
    }
   ],
   "source": [
    "# Create a list of text chunks\n",
    "\n",
    "chunk_list = list(df_data['prepared_text'])\n",
    "\n",
    "# The ids are used to create web links to each paper.\n",
    "# You can access each paper directly on ArXiv using these links:\n",
    "# https://arxiv.org/abs/{id}: ArXiv page for the paper\n",
    "# https://arxiv.org/pdf/{id}: Direct link to download the PDF\n",
    "\n",
    "arxiv_id_list = list(df_data['id'])\n",
    "cat_list = list(df_data['cat_text'])\n",
    "\n",
    "print(len(chunk_list))\n",
    "print(len(arxiv_id_list))\n",
    "print(len(cat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calculation of prompt diphoton production cross sections at Tevatron and   LHC energies {title} A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embedding vetors\n",
    "\n",
    "This step takes about 1 hour 40 minutes to create approx. 2.4 million vectors. I saved these vectors and created a Part 2 notebook where the saved vectors are loaded instead of being created from scratch.\n",
    "\n",
    "You can use the Part 2 notebook to quickly test this RAG system. There you can enter your own search queries and review the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2426574, 384)\n",
      "Embedding length 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(chunk_list)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print('Embedding length', embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity-certifying Graph Decompositions {title} We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years. In particular, our colored pebbles generalize and strengthen the previous results of Lee and Streinu and give a new proof of the Tutte-Nash-Williams characterization of arboricity. We also present a new decomposition that certifies sparsity based on the $(k,\\ell)$-pebble game with colors. Our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and Westermann and Hendrickson.\n",
      "[ 3.53147439e-03  4.11603414e-02  1.37148853e-02 -6.80273995e-02\n",
      "  7.57680973e-03 -4.09503914e-02  3.72349657e-02 -1.04655586e-01\n",
      " -3.49298641e-02  3.47316153e-02 -6.27016276e-02 -2.86809392e-02\n",
      "  7.23923147e-02  5.15273288e-02 -9.54017602e-03  6.60428107e-02\n",
      "  4.20998000e-02  1.08248899e-02 -6.30283821e-03 -2.82496382e-02\n",
      " -7.43562132e-02 -9.07442793e-02 -1.01687208e-01 -2.11284892e-03\n",
      "  5.65355830e-02 -3.50551456e-02 -2.83150878e-02  2.57018190e-02\n",
      "  3.27951722e-02 -2.72283833e-02  7.34042469e-03 -1.52552230e-02\n",
      "  2.68811118e-02 -3.96433333e-03 -4.84485598e-03  8.58275890e-02\n",
      " -4.76356633e-02 -2.05220897e-02 -4.44673859e-02  4.81818467e-02\n",
      "  1.46940853e-02  7.04236850e-02 -4.18946184e-02  2.74444092e-02\n",
      " -9.46270861e-03  6.55128360e-02 -7.64577240e-02  6.64955471e-03\n",
      " -3.36209796e-02 -6.65396154e-02 -3.07783484e-02 -7.66313821e-02\n",
      " -7.07528889e-02 -2.85267234e-02 -6.41722512e-03 -6.07522056e-02\n",
      " -8.63867404e-04 -4.70673516e-02  2.37216167e-02  1.21161323e-02\n",
      "  6.72978908e-02 -7.34724924e-02 -2.11174786e-02  2.73968503e-02\n",
      " -5.52364625e-04  2.74265055e-02  9.32949930e-02 -3.81327420e-02\n",
      " -3.53890844e-02  7.44958296e-02  6.17977530e-02  2.02309527e-02\n",
      " -1.13600614e-02  8.48671272e-02  9.89388209e-03  4.32594344e-02\n",
      "  2.70535834e-02 -3.31447497e-02 -8.79655406e-02 -5.81654254e-03\n",
      " -4.18143310e-02 -7.19662802e-03 -9.50686634e-02 -4.29127812e-02\n",
      "  3.50091606e-02 -4.24931198e-02 -6.74572960e-02  3.54162091e-03\n",
      "  7.15107517e-03 -7.78419292e-03 -5.92995621e-02  1.57937407e-02\n",
      " -2.21476387e-02  7.18067139e-02 -6.56905174e-02  9.38008539e-03\n",
      "  3.56313176e-02 -1.65483039e-02  2.09046807e-03  1.04267754e-01\n",
      " -2.72225253e-02 -8.53890367e-03  6.47930503e-02 -7.70450197e-03\n",
      "  3.46657969e-02  7.30151087e-02 -2.62231603e-02  4.73423861e-02\n",
      " -2.97965072e-02 -5.10170544e-03  5.75176664e-02  2.46276148e-02\n",
      "  2.35570781e-02  1.35034263e-01 -8.94585177e-02 -5.73109090e-02\n",
      "  9.27936193e-03 -2.42585205e-02  8.39705467e-02  5.83141483e-02\n",
      "  1.35080367e-01 -3.77927572e-02 -2.42499374e-02 -2.84446087e-02\n",
      "  7.18552172e-02  2.73540942e-03  2.71272380e-03  1.45478308e-33\n",
      "  9.82718244e-02  4.75378036e-02  5.77787422e-02  1.64650679e-02\n",
      "  9.13633257e-02 -4.21456434e-02  2.66629434e-03 -1.23954127e-02\n",
      " -2.43456475e-03 -1.14976196e-02 -7.68848136e-02  2.27545835e-02\n",
      "  2.59480905e-03  3.48469615e-02  6.15612194e-02 -4.26105671e-02\n",
      "  6.65165111e-02 -9.47573595e-03  2.85166912e-02 -6.65517747e-02\n",
      "  4.60908487e-02  1.56185664e-02  8.65443517e-03 -5.63351326e-02\n",
      " -1.13704083e-02  1.23454919e-02  2.46283486e-02 -1.00043871e-01\n",
      "  1.03629055e-02  4.32883529e-03 -6.02695756e-02 -2.27268841e-02\n",
      "  1.19475815e-02  1.54718265e-01  1.48915490e-02  2.07176358e-02\n",
      " -2.80058533e-02 -4.99044769e-02  9.29785427e-03 -1.04085691e-02\n",
      " -2.22744141e-03  8.12562928e-03 -4.39190585e-03  6.71639806e-03\n",
      " -4.51123603e-02 -1.32152038e-02  4.30057645e-02  9.20011289e-03\n",
      " -4.07233350e-02  2.17295587e-02  2.37366669e-02 -3.90583053e-02\n",
      "  9.27237980e-03  3.18523720e-02  1.82276517e-02 -5.15170395e-02\n",
      "  3.82492058e-02 -2.00191494e-02 -3.90057378e-02  1.92210209e-02\n",
      "  4.41129170e-02  2.05061678e-02  5.39818369e-02 -1.51919230e-05\n",
      " -1.05017340e-02  5.04516736e-02 -5.88518679e-02 -4.86564413e-02\n",
      "  2.83455532e-02 -7.92476535e-02 -6.40975758e-02  4.89842370e-02\n",
      "  2.84658168e-02 -4.37298566e-02  3.00923083e-02 -3.05937510e-02\n",
      "  9.79079604e-02 -1.28557667e-01 -6.93522543e-02 -4.06814814e-02\n",
      " -9.30714309e-02 -6.77385181e-02 -2.68912688e-02 -9.71834362e-02\n",
      " -1.27207682e-01 -6.35439456e-02  6.78227097e-02  3.76669168e-02\n",
      "  1.98442880e-02 -8.29102248e-02 -7.73299038e-02  1.97289828e-02\n",
      " -4.18148078e-02 -3.34300175e-02  4.17958908e-02 -2.39200794e-33\n",
      " -6.53473735e-02 -2.68325619e-02  1.43808872e-02  1.09195143e-01\n",
      "  1.55067574e-02 -7.21156746e-02 -2.51002591e-02 -1.07115097e-02\n",
      " -2.42060293e-02 -1.95615292e-02 -9.50922891e-02  7.57509992e-02\n",
      "  6.10141717e-02  1.10595794e-02 -1.64575521e-02  2.74553746e-02\n",
      " -2.62879226e-02  1.57561824e-02 -5.76560348e-02 -8.79117101e-03\n",
      " -5.75356707e-02  1.09681748e-01 -2.80207060e-02 -4.26877439e-02\n",
      "  1.15487702e-01  1.73525065e-02 -4.27734926e-02 -2.51087602e-02\n",
      " -3.81508959e-03  7.38993436e-02  2.19031051e-02 -1.07321672e-01\n",
      " -3.58806849e-02 -3.97426523e-02  1.79273169e-02  3.27319205e-02\n",
      "  3.86100858e-02  4.34580073e-02 -1.98272523e-03  6.29050583e-02\n",
      " -5.33763273e-03 -2.65330970e-02 -9.48078409e-02  6.41333237e-02\n",
      " -1.37238363e-02  5.91909662e-02 -5.24861366e-03  7.12241279e-03\n",
      " -3.13944183e-02 -3.37203522e-03  9.49270278e-03  3.57515030e-02\n",
      "  6.08111732e-02  8.64495523e-03  2.38545667e-02 -5.54941781e-02\n",
      " -1.63801964e-02  5.64686507e-02 -3.22947018e-02 -1.36128375e-02\n",
      " -4.29646671e-02  2.09400784e-02 -1.31638020e-01  9.66078341e-02\n",
      "  4.98991534e-02  7.00588198e-03 -2.59826761e-02 -5.07867150e-02\n",
      " -5.42880148e-02  2.30406504e-02 -4.23268136e-03  4.97039370e-02\n",
      " -6.20857514e-02 -3.57759045e-03  5.22820652e-02  7.07923472e-02\n",
      "  1.18809089e-03  9.03206021e-02 -6.24879934e-02  6.45548031e-02\n",
      " -2.36224104e-02  2.65063141e-02  4.57410291e-02  2.47914949e-03\n",
      " -9.83314589e-03  4.94296327e-02  1.29278703e-02  1.38363475e-02\n",
      "  1.98927354e-02 -2.93927011e-03  8.19012895e-02  1.85588282e-02\n",
      "  8.49022418e-02 -1.47474715e-02  2.64351554e-02 -4.11837107e-08\n",
      " -8.05824175e-02  4.14607003e-02 -2.85341293e-02 -4.37662005e-02\n",
      "  9.21069607e-02 -4.22161967e-02  1.38059735e-01 -1.72029866e-03\n",
      " -4.08920832e-02  4.66761068e-02  6.88366517e-02 -2.97080595e-02\n",
      " -1.73533186e-02  4.59854491e-03  2.06350181e-02  1.22707989e-02\n",
      "  9.96011496e-03  2.34046276e-03 -2.55198516e-02  1.13922112e-01\n",
      " -5.84374852e-02 -2.09800396e-02 -5.25337495e-02  5.68359308e-02\n",
      " -5.36464006e-02 -2.57595945e-02 -5.84659874e-02  8.44461960e-04\n",
      "  4.85062413e-02  6.70678169e-02 -1.54672628e-02  2.80160271e-02\n",
      "  1.25706792e-01 -7.32412282e-03 -4.02602367e-03  1.42800778e-01\n",
      " -1.74810458e-02  8.29007402e-02 -1.07233196e-01 -1.60268713e-02\n",
      " -1.23851262e-01  1.86400414e-02 -5.54206893e-02 -6.28030300e-02\n",
      "  2.81571820e-02  8.13298766e-03 -2.50843335e-02 -2.44405121e-02\n",
      " -2.30421219e-03 -8.18549935e-03 -7.83229917e-02 -1.57951675e-02\n",
      " -5.17456681e-02 -4.66634557e-02  3.78839634e-02  3.43365595e-02\n",
      " -3.99484746e-02  9.34394915e-03  1.19812258e-01 -3.96649018e-02\n",
      " -2.12510629e-03 -4.83160978e-03  5.41204261e-03  2.30322201e-02]\n"
     ]
    }
   ],
   "source": [
    "# Display one embedding\n",
    "\n",
    "i = 1\n",
    "print(chunk_list[i])\n",
    "print(embeddings[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the embedding vectors and the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_packages_and_introduction.ipynb\n",
      "Arxiv_RAG.ipynb\n",
      "Build_an_ArXiv_RAG_search_system_w_FAISS.ipynb\n",
      "LICENSE\n",
      "\u001b[34marxiv\u001b[m\u001b[m\n",
      "compressed_array.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Save the array in compressed format\n",
    "np.savez_compressed('compressed_array.npz', array_data=embeddings)\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 3295.402711868286 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the saved file\n",
    "\n",
    "import os\n",
    "\n",
    "# Get the size of the file in bytes\n",
    "file_size_bytes = os.path.getsize('compressed_array.npz')\n",
    "\n",
    "# Convert bytes to megabytes\n",
    "file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(\"File size:\", file_size_mb, \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2426574, 384)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to load the saved array\n",
    "\n",
    "# Load the compressed array\n",
    "loaded_embeddings = np.load('compressed_array.npz')\n",
    "\n",
    "# Access the array by the name you specified ('my_array' in this case)\n",
    "loaded_embeddings = loaded_embeddings['array_data']\n",
    "\n",
    "loaded_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_packages_and_introduction.ipynb\n",
      "Arxiv_RAG.ipynb\n",
      "Build_an_ArXiv_RAG_search_system_w_FAISS.ipynb\n",
      "LICENSE\n",
      "\u001b[34marxiv\u001b[m\u001b[m\n",
      "compressed_array.npz\n",
      "compressed_dataframe.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame in compressed format\n",
    "\n",
    "df_data.to_csv('compressed_dataframe.csv.gz', compression='gzip', index=False)\n",
    "\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2426574, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c0/s3dff31j48j6qs83tjklr6kw0000gn/T/ipykernel_17590/2214504676.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('compressed_dataframe.csv.gz', compression='gzip')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>cat_text</th>\n",
       "      <th>prepared_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0001</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>A fully differential calculation in perturbati...</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>High Energy Physics - Phenomenology</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0002</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-pe...</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>Combinatorics, Computational Geometry</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions {titl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  704.0001  Calculation of prompt diphoton production cros...   \n",
       "1  704.0002           Sparsity-certifying Graph Decompositions   \n",
       "\n",
       "                                            abstract     categories  \\\n",
       "0  A fully differential calculation in perturbati...         hep-ph   \n",
       "1  We describe a new algorithm, the $(k,\\ell)$-pe...  math.CO cs.CG   \n",
       "\n",
       "                                cat_text  \\\n",
       "0    High Energy Physics - Phenomenology   \n",
       "1  Combinatorics, Computational Geometry   \n",
       "\n",
       "                                       prepared_text  \n",
       "0  Calculation of prompt diphoton production cros...  \n",
       "1  Sparsity-certifying Graph Decompositions {titl...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('compressed_dataframe.csv.gz', compression='gzip')\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to set up FAISS for Exhaustive Search\n",
    "\n",
    "In an exhaustive search (brute-force search) we compare the query vector to every vector in the database. Therefore, we don't need to train an index.\n",
    "\n",
    "You'll need to have watched this video to understand all that we are going to do next:<br>\n",
    "Faiss - Introduction to Similarity Search<br>\n",
    "https://www.youtube.com/watch?v=sKyvsdEv6rk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "embed_length = embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(embed_length)\n",
    "\n",
    "# Check if the index is trained.\n",
    "# No training needed when using greedy search i.e. IndexFlatL2\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2426574"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the embeddings to the index\n",
    "\n",
    "index.add(embeddings)\n",
    "\n",
    "# Check the total number of embeddings in the index\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1734586 1564281 1753586]]\n",
      "[[0.83057106 0.849064   0.85731745]]\n"
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "\n",
    "query_text = \"\"\"\n",
    "I want to quantify the data that can be reconstructed from gradients of trained model\n",
    "\"\"\"\n",
    "query = [query_text]\n",
    "\n",
    "\n",
    "# Vectorize the query string\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Set the number of outputs we want\n",
    "top_k = 3\n",
    "\n",
    "# Run the query\n",
    "# index_vals refers to the chunk_list index values\n",
    "scores, index_vals = index.search(query_embedding, top_k)\n",
    "\n",
    "print(index_vals)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analysing Training-Data Leakage from Gradients through Linear Systems   and Gradient Matching {title} Recent works have demonstrated that it is possible to reconstruct training images and their labels from gradients of an image-classification model when its architecture is known. Unfortunately, there is still an incomplete theoretical understanding of the efficacy and failure of these gradient-leakage attacks. In this paper, we propose a novel framework to analyse training-data leakage from gradients that draws insights from both analytic and optimisation-based gradient-leakage attacks. We formulate the reconstruction problem as solving a linear system from each layer iteratively, accompanied by corrections using gradient matching. Under this framework, we claim that the solubility of the reconstruction problem is primarily determined by that of the linear system at each layer. As a result, we are able to partially attribute the leakage of the training data in a deep network to its architecture. We also propose a metric to measure the level of security of a deep learning model against gradient-based attacks on the training data.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the first search result\n",
    "\n",
    "pred_indexes = index_vals[0]\n",
    "\n",
    "i = 0\n",
    "chunk_index = pred_indexes[i]\n",
    "text = chunk_list[chunk_index]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up FAISS - Nearest Neighbor Search\n",
    "\n",
    "Exhaustive (brute-force) search can be slow when searching over a large number of vectors. A nearest neighbor search is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many clusters (voronoid cells) do we want?\n",
    "# Example: For 4 centroilds we need at least 156 embeddings in\n",
    "# order to train the index.\n",
    "num_centroids = 5\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(embed_length)\n",
    "\n",
    "index = faiss.IndexIVFFlat(quantizer, embed_length, num_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the index\n",
    "# After the index is trained it's ready to receive data\n",
    "\n",
    "index.train(embeddings)\n",
    "\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embeddings to the index\n",
    "\n",
    "index.add(embeddings)\n",
    "\n",
    "# Check how many embeddings are in the index\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [query_text]\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "\n",
    "# Run the query\n",
    "# index_vals refers to the chunk_list index values\n",
    "scores, index_vals = index.search(query_embedding, top_k)\n",
    "\n",
    "print(index_vals)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the first search result\n",
    "\n",
    "pred_indexes = index_vals[0]\n",
    "\n",
    "i = 3\n",
    "chunk_index = pred_indexes[i]\n",
    "text = chunk_list[chunk_index]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reverseengineer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
